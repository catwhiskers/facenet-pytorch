{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Face detection and recognition inference pipeline\n",
    "\n",
    "The following example illustrates how to use the `facenet_pytorch` python package to perform face detection and recogition on an image dataset using an Inception Resnet V1 pretrained on the VGGFace2 dataset.\n",
    "\n",
    "The following Pytorch methods are included:\n",
    "* Datasets\n",
    "* Dataloaders\n",
    "* GPU/CPU processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from facenet_pytorch import MTCNN, InceptionResnetV1\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "workers = 0 if os.name == 'nt' else 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Determine if an nvidia GPU is available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "print('Running on device: {}'.format(device))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define MTCNN module\n",
    "\n",
    "Default params shown for illustration, but not needed. Note that, since MTCNN is a collection of neural nets and other code, the device must be passed in the following way to enable copying of objects when needed internally.\n",
    "\n",
    "See `help(MTCNN)` for more details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mtcnn = MTCNN(\n",
    "    image_size=160, margin=0, min_face_size=20,\n",
    "    thresholds=[0.6, 0.7, 0.7], factor=0.709, post_process=True,\n",
    "    device=device, keep_all = True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define Inception Resnet V1 module\n",
    "\n",
    "Set classify=True for pretrained classifier. For this example, we will use the model to output embeddings/CNN features. Note that for inference, it is important to set the model to `eval` mode.\n",
    "\n",
    "See `help(InceptionResnetV1)` for more details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resnet = InceptionResnetV1(pretrained='vggface2').eval().to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define a dataset and data loader\n",
    "\n",
    "We add the `idx_to_class` attribute to the dataset to enable easy recoding of label indices to identity names later one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image, ImageDraw\n",
    "aligned_faces = []\n",
    "picture_tracked = []\n",
    "detected_faces = [] \n",
    "image_paths = [\"../drama/drama_01\",\"../drama/drama_02\"] \n",
    "for p in image_paths:\n",
    "    for img in os.listdir(p):\n",
    "        im = Image.open(os.path.join(p, img))\n",
    "        im = im.convert('RGB')\n",
    "        boxes, _ = mtcnn.detect(im)\n",
    "        aligned, prob= mtcnn(im, return_prob=True)\n",
    "        if aligned is not None:\n",
    "            for a in aligned: \n",
    "                aligned_faces.append(a)\n",
    "        if boxes is None:\n",
    "            continue\n",
    "        frame_draw = im.copy()\n",
    "        draw = ImageDraw.Draw(frame_draw)\n",
    "        for box in boxes:\n",
    "            draw.rectangle(box.tolist(), outline=(255, 0, 0), width=6)\n",
    "            face = frame_draw.crop(box)\n",
    "            detected_faces.append(face)\n",
    "        picture_tracked.append(frame_draw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython import display\n",
    "\n",
    "d = display.display(picture_tracked[0], display_id=True)\n",
    "i = 1\n",
    "try:\n",
    "    while i < len(picture_tracked):\n",
    "        d.update(picture_tracked[i % len(picture_tracked)])\n",
    "        i += 1\n",
    "except KeyboardInterrupt:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for face in detected_faces:\n",
    "    display.display(face, display_id=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# names = [\"新垣結衣\", \"配角1\", \"星野源\", \"新垣結衣2\", \"配角2\",\n",
    "#          \"配角3\", \"星野源2\", \"星野源3\", \"新垣結衣3\", \"新垣結衣4\",\n",
    "#          \"配角2-2\", \"星野源4\", \"新垣結衣5\", \"星野源5\",\"新垣結衣6\", \"吉高由里子\",\n",
    "#          \"配角4\", \"吉高由里子2\",\"配角4\",\"內田有紀\",\"吉高由里子3\",\"內田有紀2\",\"吉高由里子4\",\n",
    "#          \"向井理\",\"吉高由里子5\", \"配角5\", \"配角6\",\"內田有紀3\"]\n",
    "\n",
    "\n",
    "names = [\"Aragaki_Yui\", \"co_star1\", \"Hoshino_Gen\", \"Aragaki_Yui2\", \"co_star2\",\n",
    "         \"co_star3\", \"Hoshino_Gen2\", \"Hoshino_Gen3\", \"Aragaki_Yui3\", \"Aragaki_Yui4\",\n",
    "         \"co_star2-2\", \"Hoshino_Gen4\", \"Aragaki_Yui5\", \"Hoshino_Gen5\",\"Aragaki_Yui6\", \"Yuriko_Yoshitaka\",\n",
    "         \"co_star4\", \"Yuriko_Yoshitaka2\",\"co_star4\",\"Yuki_Uchida\",\"Yuriko_Yoshitaka3\",\"Yuki_Uchida2\",\"Yuriko_Yoshitaka4\",\n",
    "         \"Mukai_Osamu\",\"Yuriko_Yoshitaka5\", \"co_star5\", \"co_star6\",\"Yuki_Uchida3\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calculate image embeddings\n",
    "\n",
    "MTCNN will return images of faces all the same size, enabling easy batch processing with the Resnet recognition module. Here, since we only have a few images, we build a single batch and perform inference on it. \n",
    "\n",
    "For real datasets, code should be modified to control batch sizes being passed to the Resnet, particularly if being processed on a GPU. For repeated testing, it is best to separate face detection (using MTCNN) from embedding or classification (using InceptionResnetV1), as calculation of cropped faces or bounding boxes can then be performed a single time and detected faces saved for future use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aligned = torch.stack(tuple(aligned_faces)).to(device)\n",
    "\n",
    "embeddings = resnet(aligned).detach().cpu()\n",
    "embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Print distance matrix for classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dists = [[(e1 - e2).norm().item() for e2 in embeddings] for e1 in embeddings]\n",
    "df = pd.DataFrame(dists, columns=names, index=names)\n",
    "df "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.pyplot import figure\n",
    "\n",
    "figure(figsize=(10, 10), dpi=80)\n",
    "plt.yticks(np.arange(0.5, len(df.index), 1), df.index)\n",
    "plt.xticks(np.arange(0.5, len(df.columns), 1), df.columns)\n",
    "plt.xticks(rotation = 60) \n",
    "plt.pcolor(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install requests \n",
    "!pip install requests-aws4auth\n",
    "!pip install Elasticsearch==7.12.1\n",
    "!pip install urllib3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from elasticsearch import Elasticsearch, RequestsHttpConnection\n",
    "from requests_aws4auth import AWS4Auth\n",
    "import boto3\n",
    "\n",
    "host = 'search-face-recog-sd5rhxmhulra6lqh67sxcp5nxi.us-west-2.es.amazonaws.com' # For example, my-test-domain.us-east-1.es.amazonaws.com\n",
    "region = 'us-west-2' # e.g. us-west-1\n",
    "\n",
    "service = 'es'\n",
    "credentials = boto3.Session().get_credentials()\n",
    "\n",
    "\n",
    "\n",
    "es = Elasticsearch(\n",
    "    hosts = [{'host': host, 'port': 443}],\n",
    "    http_auth = ('root','Peggy@@0218'),\n",
    "    use_ssl = True,\n",
    "    verify_certs = True,\n",
    "    connection_class = RequestsHttpConnection\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# es.indices.delete(index='faces', ignore=[400, 404])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn_index = {\n",
    "    \"settings\": {\n",
    "        \"index.knn\": True\n",
    "    },\n",
    "    \"mappings\": {\n",
    "        \"properties\": {\n",
    "            \"face_vector\": {\n",
    "                \"type\": \"knn_vector\",\n",
    "                \"dimension\": 512\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "es.indices.create(index=\"faces\",body=knn_index,ignore=400)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def es_import(vector, celebid, id):\n",
    "    es.index(index='faces',\n",
    "             id=id, \n",
    "             body={\"face_vector\": vector, \n",
    "                   \"celebid\":celebid})\n",
    "        \n",
    "# es_import([0 for i in range(0, 256)], \"q1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx, (name, vector) in enumerate(zip(names, embeddings)): \n",
    "    es_import(vector.tolist(), name, idx)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def post(vector):\n",
    "    res = es.search(index=\"faces\",\n",
    "                    body={\n",
    "                        \"size\": 5,\n",
    "                            \"_source\": {\n",
    "                                \"exclude\": [\"face_vector\"]\n",
    "                            },\n",
    "                            \"min_score\": 0.3,\n",
    "                            \"query\": {\n",
    "                                \"knn\": {\n",
    "                                    \"face_vector\": {\n",
    "                                        \"vector\": vector,\n",
    "                                        \"k\": 5\n",
    "                                    }\n",
    "                                }\n",
    "                            }\n",
    "                    })\n",
    "    return res\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p=\"../drama/\"\n",
    "test_aligned = []\n",
    "for img in ['test1.jpeg', 'test2.jpg']:\n",
    "    im = Image.open(os.path.join(p, img))\n",
    "    im = im.convert('RGB')\n",
    "    boxes, _ = mtcnn.detect(im)\n",
    "    aligned, prob= mtcnn(im, return_prob=True)\n",
    "    for b in boxes: \n",
    "        display.display(im.crop(b), display_id=True)\n",
    "\n",
    "    aligned = torch.stack(tuple(aligned)).to(device)\n",
    "    embeddings = resnet(aligned).detach().cpu()\n",
    "    for emb in embeddings:\n",
    "        result = post(emb.tolist())\n",
    "        print(result)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p36",
   "language": "python",
   "name": "conda_pytorch_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
